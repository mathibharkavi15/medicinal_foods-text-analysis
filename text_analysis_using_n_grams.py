# -*- coding: utf-8 -*-
"""Text Analysis using N-grams

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ykL2FppDCrHJ9hrblEUxXbY2bmEAyvyl

# Text Analysis using N-grams

Gomathi A

> 1a. Import and install required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd 
from sklearn.model_selection import train_test_split 

import nltk
from nltk.corpus import stopwords
from nltk.classify import SklearnClassifier

from wordcloud import WordCloud,STOPWORDS
import matplotlib.pyplot as plt
# %matplotlib inline

from subprocess import check_output
#Downloading required libraries in nltk
import nltk
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('vader_lexicon')
nltk.download('punkt')

!pip install PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Importing necessary libraries
import pandas as pd
import numpy as np
import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import RegexpTokenizer
import re
import string
import random
from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt
# %matplotlib inline

"""

> 1b. Read input file allreviews.csv

"""

#Reading and displaying (few) records from allreviews.csv
allreviews_df = pd.read_csv("allreviews.csv",sep=",")
allreviews_df.head(100)

#Count total number of input records
totalcount = allreviews_df.count
print("Total reviews in ", totalcount)

#Drop duplicate records with subset "ASIN", "ProductName","ProductFlavor","ReviewId", "ReviewContent"
allreview_df.drop_duplicates(subset=["ASIN", "ProductName","ProductFlavor","ReviewId", "ReviewContent"], keep='first', inplace=True)

#Drop empty reviews
allreview_df.dropna(subset=['ReviewContent'], inplace=True)

#Display total number of records in input after removing duplicates
totalcount = allreview_df.count
print("Total reviews in ", totalcount)

import string
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.tokenize import WhitespaceTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Function to clean ReviewContent
def clean_text(text):
    # lower text
    text = text.lower()
    # tokenize text and remove puncutation
    text = [word.strip(string.punctuation) for word in text.split(" ")]
    # remove words that contain numbers
    text = [word for word in text if not any(c.isdigit() for c in word)]
    # remove stop words
    stop = stopwords.words('english')
    text = [x for x in text if x not in stop]
    # remove empty tokens
    text = [t for t in text if len(t) > 0]
    # lemmatize text
    #text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]
    lemmatizer=WordNetLemmatizer()
    text=[lemmatizer.lemmatize(x) for x in text]
    # remove words with only one letter
    text = [t for t in text if len(t) > 1]
    # join all
    text = " ".join(text)
    return(text)

# clean text data
allreview_df["ReviewContent_Clean"] = allreview_df["ReviewContent"].apply(lambda x: clean_text(x))

#Renaming the index column
allreview_df.index.names = ['allreview_idx']

#Display few input records
allreview_df.head(5)

# Calculating the word count for Review description
allreview_df['word_count'] = allreview_df['ReviewContent_Clean'].apply(lambda x: len(str(x).split()))# Plotting the word count
allreview_df['word_count'].plot(
    kind='hist',
    bins = 500,
    figsize = (12,8),title='Word Count Distribution for Review descriptions')

#Part of speech tagging for review comments
from textblob import TextBlob
blob = TextBlob(str(allreview_df['ReviewContent_Clean']))
pos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])
pos_df = pos_df.pos.value_counts()[:20]
pos_df.plot(kind = 'bar', figsize=(10, 8), title = "Top 20 Part-of-speech tagging for comments")

#Converting text descriptions into vectors using TF-IDF using Bigram
tf = TfidfVectorizer(ngram_range=(2, 2), stop_words='english', lowercase = False)
tfidf_matrix = tf.fit_transform(allreview_df['ReviewContent_Clean'])
total_words = tfidf_matrix.sum(axis=0) 
#Finding the word frequency
freq = [(word, total_words[0, idx]) for word, idx in tf.vocabulary_.items()]
freq =sorted(freq, key = lambda x: x[1], reverse=True)
#converting into dataframe 
bigram = pd.DataFrame(freq)
bigram.rename(columns = {0:'bigram', 1: 'count'}, inplace = True) 
#Taking first 20 records
bigram = bigram.head(20)

#Plotting the bigram distribution
bigram.plot(x ='bigram', y='count', kind = 'bar', title = "Bigram disribution for the top 20 words in the review description", figsize = (15,7), )

#Converting text descriptions into vectors using TF-IDF using Trigram
tf = TfidfVectorizer(ngram_range=(3, 3), stop_words='english', lowercase = False)
tfidf_matrix = tf.fit_transform(allreview_df['ReviewContent_Clean'])
total_words = tfidf_matrix.sum(axis=0) 
#Finding the word frequency
freq = [(word, total_words[0, idx]) for word, idx in tf.vocabulary_.items()]
freq =sorted(freq, key = lambda x: x[1], reverse=True)#converting into dataframe 
trigram = pd.DataFrame(freq)
trigram.rename(columns = {0:'trigram', 1: 'count'}, inplace = True) 
#Taking first 20 records
trigram = trigram.head(20)

#Plotting the trigramn distribution
trigram.plot(x ='trigram', y='count', kind = 'bar', title = "Trigram disribution for the top 20 words in the review description", figsize = (15,7), )

#Converting text descriptions into vectors using TF-IDF using N-gram
tf = TfidfVectorizer(ngram_range=(4, 4), stop_words='english', lowercase = False)
tfidf_matrix = tf.fit_transform(allreview_df['ReviewContent_Clean'])
total_words = tfidf_matrix.sum(axis=0) 
#Finding the word frequency
freq = [(word, total_words[0, idx]) for word, idx in tf.vocabulary_.items()]
freq =sorted(freq, key = lambda x: x[1], reverse=True)   #converting into dataframe 
ngram = pd.DataFrame(freq)
ngram.rename(columns = {0:'ngram', 1: 'count'}, inplace = True) 
#Taking first 20 records
ngram1 = ngram.head(20)

#Plotting the n-gram distribution
ngram1.plot(x ='ngram', y='count', kind = 'bar', title = "N-gram disribution for the top 20 words in the review description", figsize = (15,7), )